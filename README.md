# ğŸ¯ Manga AI Translator

An automated, privacy-focused, GPU-accelerated pipeline to translate manga and comics locally.

This project aims to provide a full-stack solution (Frontend, Backend, and AI Worker) to detect text bubbles, perform OCR, translate contextually using LLMs, and typeset the result back into the original imageâ€”all without external APIs or recurring costs.

## ğŸ—ï¸ Architecture

The project follows a Microservices architecture to ensure the heavy AI processing doesn't block the web server.

```mermaid
graph LR
    User[User<br/>Web Client]
    API[Backend<br/>API]
    Storage[(Storage)]
    Queue[Message<br/>Queue]

    User -->|Upload| API
    API -->|Store| Storage
    API -->|Queue Job| Queue

    subgraph AIW["AI Worker - Python"]
        Worker[Worker<br/>Service]
        YOLO[YOLO<br/>Detection]
        OCR[Manga<br/>OCR]
        LLM[Qwen 2.5<br/>Translation]
        Typeset[Typesetting<br/>Engine]

        Worker -->|1. Detect| YOLO
        YOLO -->|2. Read| OCR
        OCR -->|3. Translate| LLM
        LLM -->|4. Render| Typeset
    end

    Queue -->|Pop Job| Worker
    Typeset -->|Save| Storage
    Storage -->|Download| User
```

## ğŸ§© Project Structure

| Module | Status | Description |
|--------|--------|-------------|
| `/ai-worker` | âœ… v9.0 | The core Python engine. Handles Computer Vision, OCR, and LLM Inference on GPU. |
| `/backend-api` | ğŸš§ Planned | High-performance API (Go/NestJS) to handle uploads, queues, and file serving. |
| `/frontend` | ğŸš§ Planned | Modern Web UI (React) for drag-and-drop uploads and reading translated chapters. |

## âœ¨ Key Features (AI Worker)

The core engine is currently fully operational.

- **âš¡ 100% Local & Uncensored**: Powered by llama.cpp and Abliterated models. No moralizing, just translation.
- **ğŸ‘ï¸ Smart Detection**: Uses YOLOv8 fine-tuned on Manga109 to detect speech bubbles.
  - **Feature**: Smart Box Merging automatically consolidates fragmented vertical text bubbles.
- **ğŸ“– Specialized OCR**: Uses MangaOCR to handle vertical Japanese text and handwritten fonts.
- **ğŸ§  Context-Aware Translation**:
  - Uses Qwen 2.5 7B (Instruction tuned).
  - Custom prompt engineering to handle "Subject-less" Japanese sentences.
  - "Anti-Thinking" regex filters to remove internal LLM monologues.
- **ğŸ¨ Advanced Typesetting**:
  - **Inpainting**: Cleans bubbles using context-aware rounded rectangles.
  - **Pixel-Perfect Wrapping**: Custom algorithm measuring exact pixel width of words to avoid overflow.
  - **Sanitization**: Filters out unsupported characters (emojis, math symbols) to prevent font rendering glitches.
- **ğŸ“¦ Batch Processing**: Native support for .zip archives (extract â†’ translate â†’ repack).
- **ğŸ—ï¸ Modular Architecture**: Clean, maintainable codebase with separation of concerns for easy customization and extension.

## ğŸš€ Getting Started (Worker Only)

Currently, you can run the worker as a CLI tool.

### Prerequisites

- NVIDIA GPU with 6GB+ VRAM (Recommended: 8GB+).
- CUDA Toolkit 12.x installed.
- Python 3.10+.

### Setup

1. Navigate to the worker directory:

```bash
cd ai-worker
```

2. Install dependencies (ensure CUDA support):

```bash
pip install -r requirements.txt
```

See inner README for detailed llama-cpp-python compilation instructions.

3. Run on an image or a zip file:

```bash
python main.py ../my_manga_chapter.zip
```

## ğŸ—ºï¸ Roadmap

- [x] Core AI Pipeline (Detection, OCR, Translation, Inpainting)
- [x] GPU Optimization (VRAM management, 4-bit quantization)
- [x] Smart Typesetting (Pixel wrapping, box merging)
- [x] Modular Code Architecture (Config, Services, Utils separation)
- [ ] Backend API (Go/NestJS setup, Redis integration)
- [ ] Frontend UI (React, File upload zone, Gallery)
- [ ] Docker Compose (One command deployment)

## ğŸ¤ Credits

- **Models**: Qwen (Alibaba Cloud), YOLOv8 (Ultralytics), MangaOCR (kha-white).
- **Tech**: Llama.cpp, PyTorch, Pillow.

---

**Current Version**: V9 (Stable)
